name: Run Download Pipeline

on:
  workflow_dispatch:
    inputs:
      limit:
        description: 'Number of NEW documents to download'
        required: true
        default: '5'
        type: number
      persist_database:
        description: 'Create PR with updated downloaded_files_database.csv'
        required: true
        default: true
        type: boolean
      sleep_seconds:
        description: 'Optional seconds to sleep between downloads'
        required: false
        default: '0'
        type: string

permissions:
  contents: write
  pull-requests: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: '**/pyproject.toml'

      - name: Install Python dependencies
        run: uv pip install --system -e .

      - name: Ensure output directories exist
        run: |
          mkdir -p metadata_output
          mkdir -p Downloads

      - name: Run incremental download pipeline
        run: |
          python3 run_full_pipeline.py \
            --metadata-output-dir metadata_output \
            --download-dir Downloads \
            --limit "${{ inputs.limit }}" \
            --sleep "${{ inputs.sleep_seconds }}"

      - name: Show run output summary
        run: |
          python3 - <<'PY'
          import csv
          p = 'metadata_output/latest_downloaded_metadata.csv'
          with open(p, newline='', encoding='utf-8') as f:
              r = csv.DictReader(f)
              rows = list(r)
          print('rows', len(rows))
          print('has_ContentDocumentId', 'ContentDocumentId' in (r.fieldnames or []))
          print('has_sha256', 'sha256' in (r.fieldnames or []))
          print('ids', [row.get('ContentDocumentId', '') for row in rows])
          PY

      - name: Upload run outputs as artifact
        uses: actions/upload-artifact@v4
        with:
          name: download-pipeline-output
          path: |
            metadata_output/latest_downloaded_metadata.csv
            metadata_output/downloaded_files_database.csv
          if-no-files-found: error

      - name: Detect latest parquet output
        id: latest_parquet
        run: |
          if ls pdf_parsing/parquet_files/*.parquet >/dev/null 2>&1; then
            latest=$(ls -t pdf_parsing/parquet_files/*.parquet | head -n 1)
            echo "path=${latest}" >> "$GITHUB_OUTPUT"
            echo "Latest parquet: ${latest}"
          else
            echo "path=" >> "$GITHUB_OUTPUT"
            echo "No parquet output found for this run."
          fi

      - name: Upload latest parquet as artifact
        if: ${{ steps.latest_parquet.outputs.path != '' }}
        uses: actions/upload-artifact@v4
        with:
          name: download-pipeline-latest-parquet
          path: ${{ steps.latest_parquet.outputs.path }}
          if-no-files-found: error

      - name: Create PR with updated download database
        if: ${{ inputs.persist_database }}
        uses: peter-evans/create-pull-request@v8
        with:
          commit-message: 'Update downloaded files database'
          title: 'Update downloaded files database'
          body: |
            This PR was automatically generated by the Run Download Pipeline workflow.

            It updates:
            - `metadata_output/downloaded_files_database.csv`
          branch: download-db-updates
          delete-branch: true
          add-paths: |
            metadata_output/downloaded_files_database.csv
